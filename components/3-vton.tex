\section{\textbf{Virtual try-on}} \label{section:vton}
	Virtual try-On systems represent a cutting-edge fusion of technology and fashion retail, revolutionizing the way consumers interact with clothing online. These systems utilize artificial intelligence, computer vision, and augmented reality to simulate the experience of trying on clothing virtually. Users can see how garments fit, drape, and look on their own bodies without physically trying them on. From 2D image-based try-ons to advanced 3D models, these systems offer a range of immersive experiences.
	
	The goal of virtual try-on is not only to enhance user confidence and satisfaction, but also to reduce return rates, making it a pivotal tool in modern e-commerce. It showcases the potential of technology to bridge the gap between digital and physical retail, offering consumers an engaging and informative way to make fashion choices online.

	\subsection{\textbf{Image-based (2D) virtual try-on}}
		Image-based try-on systems take in images as input data and generate images as output data. The systems infer data like pose, warp, and occlusion of the target and then use that information to guide the generation of the post-try-on image.

		\lithead{DBLP:conf/iccvw/AyushJCHK19}{
			present a multi-scale patch adversarial loss function that improves cloth fit and texture preservation in virtual try-on. A Geometric Matching Module (GMM) enhances control over shape and pose transformations, and adversarial loss promotes realistic texture and body shape propagation. Reformulation as a conditional image generation problem offers an economical virtual try-on solution without 3D information.
		} In another work \cite{DBLP:conf/iccvw/AyushJCK19}, they demonstrate how multi-task learning improves efficiency and task performance. Auxiliary tasks like semantic segmentation can enhance virtual try-on, and pseudo ground-truth masks aid in image fusion, leading to a better fit.
		
		\lithead[ClothFlow]{DBLP:conf/iccv/HanHHS19}{
			utilizes dense optical flow estimation for natural-looking clothing deformation and appearance transfer. The cascaded flow network ensures accurate appearance matching, enhancing clothed person generation and a conditional layout generator disentangles shape and appearance for spatially coherent results.
		} And in a similar vein, \lithead[UVTON]{DBLP:conf/iccvw/KuboISM19}{
			leverages UV mapping for accurate virtual try-on across diverse postures.
		} Both of these rely on DensePose integrations to enhance mapping precision by estimating 3D surface information for each pixel. However, DensePose accuracy is crucial for success and that is challenging as warping 2D image textures to predefined coordinate systems can introduce artifacts.

		\lithead[LA-VITON]{DBLP:conf/iccvw/LeeLKCP19}{
			excels in damage-free, visually appealing virtual try-on results by using a GMM and Try-On Module (TOM) to ensure seamless synthesis. A Grid interval consistency loss maintains shape and pattern consistency, and an occlusion-handling technique enhances geometric matching for superior results.
		} And \lithead[VTNFP]{DBLP:conf/iccv/YuWX19}{
			retains clothing and body details by leveraging a GAN \cite{DBLP:journals/corr/GoodfellowPMXWOCB14} and cGAN's \cite{DBLP:journals/corr/MirzaO14} powerful generative capabilities.
		}

		The use of StyleGAN \cite{DBLP:journals/pami/KarrasLA21} by \lithead{DBLP:conf/iccvw/YildirimJVB19}{
			enables high-resolution fashion model image generation, enhancing clothing shopping visualizations. Conditional StyleGAN with embedding network allows customization of outfits and poses for fashion models. Swapping style vectors in unconditional StyleGAN enhances flexibility in image generation. Further, deep pose estimator and keypoint extraction improve accuracy in representing body poses.
		}

		\lithead[CP-VTON+]{minar2020cp}{
			corrects clothing-agnostic human representation by addressing labeling and omission issues. It enhances composition masks using input clothing mask and a concrete loss function.
		} \lithead[WUTON]{DBLP:conf/eccv/IssenhuthMC20}{
			adopts a student-teacher paradigm to improve virtual try-on performance by exploiting masked information. Adversarial loss ensures the student model mimics real image distribution, enhancing realism. A Real-time virtual try-on is achieved by omitting human parser and pose estimator at inference.
		}

		A hybrid 2D-3D method proposed by \lithead{minar20203d}{
			enables accurate 3D clothing reconstruction for natural shapes. 3D deformations preserve clothing characteristics, especially for large transformations or detailed textures, and the template-based approach aligns clothing image with the silhouette of the body's 3D model for precise reconstruction. SMPL model ensures accurate representation of diverse body shapes and poses. An updated TOM network enhances blending stage for improved virtual try-on results.
		}

		\lithead[OVNet]{DBLP:conf/cvpr/LiCZL21}{
			presents a method to capture important details like buttons, shading, textures, and realistic hemlines, resulting in high-quality virtual try-on images. It introduces a technique for matching outfits with the most suitable model, leading to significant improvements in try-on results. It also consists of a semantic layout generator and an image generation pipeline using multiple coordinated warps, which yields consistent improvements in detail. But it does not handle variations in body shape, which limits its ability to dress garments directly on different body types. It also does not address the challenge of obtaining 3D measurements of garments and users.
		}

		Yet another hybrid approach is proposed in \lithead[CloTH-VTON+]{DBLP:journals/access/MinarTA21}{
			which combines 2D and 3D methods for realistic clothing deformations while preserving details. 2D methods generate disclosed human parts, ensuring visually appealing try-on outputs, while 3D cloth reconstruction allows flexibility in applying the method to different clothing styles.
		}

		\lithead[DCTON]{DBLP:conf/cvpr/GeSGY0021}{
			can produce highly-realistic try-on images by disentangling important components of virtual try-on. It utilizes perceptual loss to ensure similar CNN feature representations between the warped clothes. It disentangles clothing, skin, and background components from input images. However, it does not differentiate clothing and non-clothing regions, which can hinder the quality of results.
		}

		Successful synthesis of high-resolution virtual try-on is achieved by \lithead[VITON-HD]{DBLP:conf/cvpr/ChoiPLC21}{} It introduces a misalignment-aware normalization technique to address spatial deformation and generate properly aligned images and utilizes conditional normalization layers to preserve semantic information and apply spatially varying affine transformations.

		Another student-teacher model, \lithead[PF-AFN]{DBLP:conf/cvpr/GeSZG0021}{
			produces highly photo-realistic try-on images without relying on human parsing by the use of appearance flows between the person image and the garment image which enables accurate dense correspondences and high-quality results. It introduces an adjustable distillation loss function to ensure accurate representations and predictions. However, heavily relies on the use of a parser-based model as the `teacher' network, which may limit the image quality of the `student' network.
		}

		\lithead[FashionMirror]{DBLP:conf/iccv/ChenLHSC21}{
			proposes a co-attention feature remapping framework for virtual try-on to generate realistic results with spatio-temporal smoothness. It allows for refinement through convolution and generating different views.
		} \lithead[ZFlow]{DBLP:conf/iccv/ChopraJHK21}{
			presents an end-to-end framework addressing concerns regarding geometric and textural integrity. It incorporates gated aggregation of hierarchical flow estimates (Gated Appearance Flow), and dense structural priors to improve depth perception, handling of occlusion, and reduction of artifacts in try-on outputs.
		}

		\lithead[DiOr]{DBLP:conf/iccv/CuiML21}{
			supports various fashion-related tasks such as 2D pose transfer, virtual try-on, and outfit editing. It explicitly encodes the shape and texture of each garment, allowing for separate editing of these elements. A joint training on pose transfer and inpainting helps with detail preservation and coherence of generated garments. However, complex or rarely seen poses may not always be rendered correctly, and ghosting artifacts and improper filling of holes in garments can also be present.
		}

		\lithead[DBCT]{DBLP:conf/cvpr/FenocchiMCBCC22}{
			improves the generation of virtual try-on results by dealing with long-range dependencies and achieving more realistic and accurate results and demonstrates its effectiveness in comparison to both standard pure convolutional approaches and previous transformer-based proposals.
		}

		A cheap and scalable weakly-supervised method is proposed by \lithead{DBLP:conf/cvpr/FengMSGLLOZZ22}{
			which shows that projecting the rough alignment of clothing and body onto the StyleGAN space can yield photo-realistic wearing results. The Deep Generative Projection algorithm has a time cost for semantic and pattern searches and the method still faces difficulty against state-of-the-art methods in handling extremely complicated poses. Has an undesirably high FID score of 48.4.
		}

		\lithead{DBLP:conf/cvpr/HeSX22}{
			presents a perceptual loss and a warping model to train the try-on model, which helps improve the quality of the generated appearance flow and the final try-on results. Usage of a pre-trained VGG network \cite{DBLP:journals/corr/SimonyanZ14a} and a parser-based model in the training process enhances the accuracy and effectiveness.
		}

		Accurate photorealistic results are shown by \lithead[RT-VTON]{DBLP:conf/cvpr/YangY022}{
			for both standard and non-standard clothes. Is able to handle hard samples such as off-shoulder clothes. Semi-rigid deformation technique used in the method balances the trade-off between rigidity and flexibility of clothes warping.
		}

		\lithead[SDAFAN]{DBLP:conf/eccv/BaiZLZY22}{
			enables clothing warping and body synthesizing simultaneously by extracting and merging feature and pixel-level information from different semantic areas. It addresses challenges without relying on intermediate parser-based labels, reducing noise and inaccuracies in the try-on process.
		} \lithead[HR-VTON]{DBLP:conf/eccv/LeeGPCC22}{
			ensures information exchange and eliminates misalignment and pixel-squeezing artifacts by filtering out incorrect segmentation map predictions.
		}

		\lithead[C-VTON]{DBLP:conf/wacv/FeleLPS22}{
			generates convincing results even with subjects in difficult poses, allowing for synthesis of high-quality try-on results. However, it has issues with the masking procedure when generating image context and loose clothing. Further, it lacks the ability to differentiate between the front and backside of the target garment.
		} \lithead[3D-GCL]{DBLP:conf/nips/HuangLXKCL22}{
			incorporates 3D parametric human models as priors to better handle variations of pose and viewpoint. It utilizes 3D-aware global correspondences that encode global semantic correlations.
		}

		\lithead[TryOnDiffusion]{DBLP:conf/cvpr/ZhuYZRCS0K23}{
			generates apparel try-on results with significant body shape and pose modification by using a latent diffusion method \cite{DBLP:conf/cvpr/RombachBLEO22}. But it focuses on upper body clothing and does not experiment with full body try-on. Further, performance with more complex backgrounds is unknown.
		} \lithead[MGD]{DBLP:journals/corr/abs-2304-02051}{
			also takes this diffusion model approach and incorporates multimodal inputs such as text, body pose, and sketches and achieves good results in terms of realism and coherence with multimodal inputs. It also introduces new semi-automatically annotated datasets.
		}

		\lithead[SAL-VTON]{DBLP:conf/cvpr/YanGZX23}{
			introduces semantically associated landmarks for virtual try-on which helps in addressing misalignment issues and improving the try-on results. It also provides the ability to edit virtual try-on results using landmarks.
		} \lithead[GP-VTON]{DBLP:conf/cvpr/XieHDZDZ0L23}{
			preserves semantic information, avoids texture distortion, and handles challenging inputs by employing local flows to warp garment parts individually and dynamically truncating the gradient in the overlap area, effectively avoiding texture squeezing problems. It also easily extendeds to a multi-category scenarios.
		}

	\subsection{\textbf{Pose-guided human synthesis}}
		These systems are also image-based, but they focus on transforming a human image from reference to target pose while preserving style but changing clothing.

		\lithead{DBLP:conf/cvpr/SongZLM19} {
			propose a method that simplifies non-rigid deformation learning by using semantic parsing transformation and appearance generation tasks and attempting end-to-end training which lead to refined results. Semantic generative network helps in transforming semantic parsing maps which in turn facilitates deformation learning. Appearance generative network synthesizes semantic-aware textures for realistic person images.
		}

		For tasks like person image synthesis, sketch-to-photo and depth up-sampling, \lithead{DBLP:conf/iccv/AlbaharH19}{
			propose a conditioning scheme with diverse guidance signals and a bi-directional feature transformation for enhanced information flow.
		}

		For realistic pose transformation with precise pixel transfer, 3D appearance flow is much better. \lithead{DBLP:conf/cvpr/LiHL19}{
			observe that scarce annotations can be overcome by fitting a 3D model and projecting to 2D for dense appearence flow computation. For accurate pose transfer, synthesized gropund-truths are able to map target poses to 3D appearence flow leading to accurate pose transfer. Such workflows lead to photorealistic images of the target pose.
		}

		Two discriminators are used by \lithead{DBLP:journals/corr/abs-1906-07251}{
			to differentiate between real and generated images which ensure that the fashion item remains consistent during synthesis. Image and pose encoders provide crucial context for high-quality image generation and end-to-end training optimizes generative model parameters for synthesis of photorealistic images.
		}

		Personalized modeling and shape disentanglement is enabled by 3D body mesh recovery module. \lithead{DBLP:conf/iccv/LiuPML0G19}{
			realize that Liquid Warping GAN with Liquid Warping Block preserves vital details in both image and feature spaces and it also supports flexible warping from multiple sources which lead to diverse results.
		}

		\lithead{DBLP:conf/ijcai/HuangXCWZWHD20}{
			introduce adaptive patch normalization which improves performance through region specific normalization. It couples target pose with conditioned appearance for realistic image generation. High flexibility is observed because of decoupling and recoupling of factors. 
		}

		\lithead{DBLP:conf/cvpr/MenMJML20}{
			build an Attribute-Decomposed GAN which enabled precise control of attributes during image synthesis. The automatic separation of attributes removes manual annotations and leads to enhanced realism. The dependency on labelled data, however, may limit applicability in situations lacking readily available annotations.
		}

		GANs can be used to enhance the quality of images which is useful in realistic pose transfer. \lithead[PoNA]{DBLP:journals/tip/LiZLLD20}{
			has a long range dependency that addresses feature information gaps and self-occlusion issues. Integration of pre-posed image-guided pose feature update and post-posed image feature update improves feature utilization in the transfer process. The method generates sharper, detailed images with lesser parameters and faster speed which increased efficiency for pose transfer tasks. However, struggles with invisible areas and complex poses because of simplified block structure and posing limitations in occlusion handling are observed. Reliance on GAN is expected to cause mode collapse or instability in certain scenarios and it also faces difficulty in preserving fine details and precise transfer of textures when there are significant coordinate differences.
		}

		\lithead[SPAdaIN]{DBLP:conf/cvpr/WangWFLZXZ20}{
			extends the already 2D SPADE \cite{DBLP:conf/cvpr/Park0WZ19} approach, enabling effective pose transfer. It works directly on identity meshes, bypassing need for specific additional information. SPAdaIN ResBlocks ensure high-quality output meshes with combined pose and identity information. Geometric details are maintained by edge regularization for enhanced results. Downside to this approach is it's reliance on high quality identity meshes and pose information may hinder practical use in certain scenarios.
		}

		Combination of flow-based operations and attention is observed by \lithead{DBLP:journals/tip/RenLLL20}{
			to improve training stability and gradient propagation. It enables accurate feature-level spatial transformation, enhancing pose-guided image generation.
		}

		Enhanced accessibility and efficiency because of high-resolution appearance transfer without 3D model is attempted by \lithead{DBLP:journals/corr/abs-2008-11898}{} They utilize dense local descriptors for refined details and preserves garment textures and geometry. The progressive training in autoencoder improves generation quality at high resolutions. It realistically reproduces complex garment appearance, including occluded areas, for accurate transfer. However, global perceptual loss might not be able to preserve sharp garment textures, as noted in ablation study.

		\lithead{DBLP:journals/corr/abs-2006-01435}{
			simplify portrait editing and eliminates manual software use. It also allows simultaneous revision of posture, body figure, and clothing style through use of a GAN-based method which maintains coherency and preserves identity in recaptured portraits. The layout-map guides appearance transformation leading to enhanced accuracy. The hierarchical knowledge aids in effective recapture, especially for invisible parts.
		}

		\lithead{DBLP:journals/tog/AlBaharLYSSH21}{
			develop conditional StyleGAN which enables accurate pose-guided image synthesis. The Inpainted Correspondence Field facilitates detail transfer for drastic pose changes. The Spatially Varying Latent Space Modulation preserves local details and photo-realism.
		}

		As discussed earlier, GANs have been used to enhance image realism and shape consistency which in turn leads to appealing results, acting as high resolution data for training models. \lithead[PATN]{DBLP:journals/pami/ZhuHXSCB22}{
			demonstrates a Pose-Attentional Transfer Network which allows accurate and natural-looking pose transfers through intermediate representations. Pose distortions are reduced and image quality increases because of Perceptual L1 loss integration.
		}

		\lithead[wFlow]{DBLP:conf/cvpr/DongZXZDZLLY22}{
			is capable of transferring arbitrary garments onto challengingly-posed query person images in real-world backgrounds. It efficiently integrates the advantages of 2D pixel-flow and 3D vertex-flow. The self-supervised training scheme used in the paper leverages easily obtainable dance videos to train the model. However, since it focuses on garment transfer in the context of dance videos, it is unclear how well it performs on a large scale or with diverse datasets.
		}

		Controlled person image synthesis by blending source style with target pose is attempted by \lithead[CASD]{DBLP:conf/eccv/ZhouYCSGL22}{
			and the self-attention approach aids accurate source appearance encoding which enhances model effectiveness. Contextual loss is computed using VGG-19 features which facilitates image transformation.
		}

		\lithead[InsetGAN]{DBLP:conf/cvpr/FruhstuckSSMWL22}{
			combines pretrained GANs for diverse full-body human image generation. Specialized GANs enables seamless integration of parts which ensure high-quality results. It relies however, on pretrained GANs, which potentially, might limits its adaptability to new domains. Also, coordinating multiple generators can be complex, requiring careful tuning and coordination.
		}

	\subsection{\textbf{Multi-pose guided virtual try-on}}
		These systems are a step up from pose-guided systems; given a input image of a person, the target clothing, and a target pose, these attempt to generate the person in the target clothing in the target pose.

		\lithead{DBLP:conf/icip/HsiehCCSC19}{
			propose generation of consecutive arbitrary poses to provide more information for the user to make more informed decisions while purchasing clothes.
		} And \lithead{DBLP:conf/iccv/DongLSWLZH019}{
			introduce a three-stage approach that addressed challenges such as self-occlusions, misalignment among diverse poses, and diverse clothes texture. The misalignment between the input human pose and the target pose is alleviated by the use of deep Warp-GAN.
		}

		\lithead[3D MP-VTON]{DBLP:journals/access/ThaiMAW21}{
			allows for accurate texture mapping, which is crucial for natural clothing rendering from arbitrary views and it greatly reduces segmentation label imbalance which results in high-quality segmentation and reduced training time.
		}

		\lithead[SPG-VTON]{DBLP:journals/tmm/HuLZR22}{
			presents the use of three submodules - semantic prediction module (SPM), clothes warping module (CWM), and try-on synthesis module (TSM) working together to generate visual try-on images with preserved clothing details and desired poses.
		}

		\lithead[CF-VTON]{du2023cf}{
			addresses the challenges of preserving the person's identity and unnatural garment alignment. The proposed model includes predicting the "after-try-on" semantic map, warping the garment using an improved GAN and synthesizing a coarse result with a temporal segment network.
		}

	\subsection{\textbf{Video virtual try-on}}
		Video virtual try-on systems fit target clothes onto a person in a video with spatio-temporal consistency. This is challenging because usual image-based try-on methods cause frame-to-frame inconsistencies when applied to video data.

		\lithead[ShineOn]{DBLP:conf/wacv/KuppaJLLM21}{
			demonstrates ReLU and GeLU as the most effective activation functions, and provides clarity on quantifying the isolated visual effect of different design choices. It also specifies hyperparameter details for experimental reproduction. 
		} And \lithead[MV-VTON]{DBLP:conf/mm/ZhongWTLW21}{
			transfers desired clothes to frame images through pose alignment and region-wise pixel replacement. It embeds generated frames into the latent space as external memory for subsequent frame generation.
		}

		\lithead[ClothFormer]{DBLP:conf/cvpr/JiangWYL22}{
			generates realistic, harmonious, and spatio-temporally consistent try-on videos in complicated environments. It predicts dense flow mapping between body and clothing regions which addresses the challenge of generating accurate warping when occlusions appear in the clothing region.
		}

	\subsection{\textbf{3D virtual try-on}}
		3D virtual try-on systems reconstruct 3D meshes of the person and clothing from the target images and then fit the clothing onto the person in attempts to generate a physically accurate 3D render.

		\lithead[ULNeF]{DBLP:conf/wacv/MajithiaPBGSS22}{
			allows for mix-and-match try-on, addressing the combinatorial complexity of mixing different garments. It works directly on neural implicit representations, which can bring a change of paradigm and open the door to radically different approaches in virtual try-on. However, it has only been validated with garments in T-pose and lacks other variations.
		}

		The use of fixed topology parametric template mesh models by \lithead{DBLP:conf/nips/SantestebanOTC22}{
			for known types of garments allows for easy mapping of high-quality texture from input catalog images to UV map panels. The proposed pipeline is compact and scalable which makes it suitable for practical implementation.
		}

	\subsection{\textbf{Augmented reality try-on}}
		Augmented reality virtual try-on is the eventual goal of all try-on systems, integrating computer-generated imagery with real-world views, enabling users to virtually try on clothing and accessories in real-time.

		The use of simple video dataset to successfully achieve transfer of clothing segmentation rather than using the DeepFashion dataset is proposed by \lithead{DBLP:conf/ieeehpcs/JongM19}{} But the use of short video datasets may limit dataset size and diversity.

		\lithead{di2020comparative}{
			proposes the use of four neural models: Fully Connected Neural Network, CNN, MobileNetV1 \cite{DBLP:journals/corr/HowardZCKWWAA17}, and MobileNetV2 \cite{DBLP:conf/cvpr/SandlerHZZC18}, to classify clothing images for computationally limited platforms on which augmented reality is often implemented.
		}

		\lithead{hashmi2020augmented}{
			introduce a computationally inexpensive technique using web cam input and Haar cascade classifier \cite{DBLP:conf/cvpr/ViolaJ01} that helps avoid the use of costly kinect sensors. But the sample size used limits generalizability to broader populations. Also, Haar cascade classifier may have limitations in accurately detecting specific body parts, especially under varying lighting conditions and poses.
		}

		Providing the study of using AR to help online shoppers choose their correct size and help them choose the best option based upon various attributes Stimulus-Organism-Response model. \lithead{baytar2020evaluating}{
			investigate how AR products are perceived by consumers but the limited dataset reduces the generalizability of the findings. Another limitation is that the garments were 2D and did not wrap around the body.
		}

		\lithead[AVATAR]{shaw2020advanced}{
			Advanced Virtual Apparel Try using Augmented Reality (AVATAR) is capable of providing virtual apparel trial using AR. It provides a hardware solution using a multi-sensor body scanner for precise body coordinates. It also facilitates face color recognition, providing personalized apparel recommendations based on skin tone. But no insights about real-world implementation challenges are provided and no information regarding accuracy and reliability of sensors is discussed.
		}

		\lithead{ali2021augmented}{
			explore the use of smartphone camera to provide 3d image of product using AR which provides real time AR by using a camera and YOLO for detection. But using YOLO introduces constraints on bounding box predictions, impacting small object detection and unusual configurations. Also, selective search in Faster R-CNN \cite{DBLP:journals/pami/RenHG017} can be time-consuming, affecting overall performance.
		}

		\lithead{feng2021personalized}{
			propose the use of AR, Azure Kinect somatosensory, and OpenGL 3D rendering to provide virtual try on experience and enhance clothing customization.
		} And \lithead{moriuchi2021engagement}{
			provide an insight about use of chatbots and AR technology in e-commerce. But the included study based upon small sample size of 68 millennials may limit generalizability.
		}

		\lithead{DBLP:journals/sensors/BattistoniGRSVB22}{
			explore the role of AR as meta-user interface. It also covers a case study on focus group that validates design patterns, providing insights for improvements.
		}

	\subsection{\textbf{Commercial uses of virtual try-on}}
		Companies are also using virtual try-on systems to provide enhanced experience to customers. Table \ref{table:commercial-vton} lists the various commercially available virtual try-on platforms.

		\newcommand{\commrow}[3]{
			#2 & \cite{#1} & #3 \\ \addlinespace
		}

		\begin{table}
			\caption{Commercial virtual try-on}
			\label{table:commercial-vton}
			\begin{tabularx}{\columnwidth}{
				>{\raggedleft\arraybackslash}p{0.5cm} X p{4.25cm}
			}
				\toprule
					\textbf{Tech.} &
					\textbf{Item type} &
					\textbf{Limitations} \\
				\midrule
					\textbf{2D} & Clothing \& Accessories \cite{WalmartA, WalmartB, GoogleShopping} & AI-generated variations of models wearing garments are not true representations of the user trying on the garment. \\
				\midrule
					\textbf{AR} & Clothing \& Accessories \cite{Snapchat} & \multirow{3}{4.25cm}{\justifying Most implementations only focus on accessories instead of actual clothing items.} \\
					& Clothing \cite{Zalando} & \\
					& Wrist Watches \cite{BaumeMercier} & \\
					& Eyewear \cite{WarbyParker} & \\
					& Makeup \cite{YTAR} & \\
					& Makeup \cite{LOreal} & \\
				\bottomrule
			\end{tabularx}
		\end{table}
